{
    "wss": {
        "host": "127.0.0.1", // or "localhost" but might cause issues due to ipv6
        "port": 4286
    },
    "openllm": {
        "base_url": "https://api.openai.com/v1", // change for use with openai-compatible backend
        "model": "gpt-4o-mini",
        "temp": 1.0,
        "max_completion_tokens": 1000
    },
    "short_vdb": {
        "progressive_eviction": true, // will evict older memories once limit is reached
        "max_size_before_evict": 500  // used if progressive_eviction==false, else no limit
    },
    "long_vdb": {
        "max_size": 5000
    },
    "user_db": {
        "max_size_per_user": 100
    }
}
